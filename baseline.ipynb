{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "113iSZNe61dd"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install datasets\n",
        "!pip install accelerate -U\n",
        "from accelerate import Accelerator\n",
        "!pip show accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIEjNvgw7GJE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
        "import re\n",
        "import random\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "!pip download transformers==4.39.3\n",
        "!pip download tokenizers==0.15.2\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import DataCollatorWithPadding\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import log_loss\n",
        "from tokenizers import AddedToken\n",
        "warnings.simplefilter('ignore')\n",
        "!pip show transformers\n",
        "!pip show tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKdUhp0H7Hyu"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# Directory settings\n",
        "# ====================================================\n",
        "OUTPUT_DIR = './'\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3FA7TgxH7Q5d"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/kaggle/input/lmsys-chatbot-arena/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
        "submission = pd.read_csv('/content/drive/MyDrive/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lHaPwNv77TXf"
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "    n_splits = 5\n",
        "    seed = 42\n",
        "    max_length = 1539 # 512 x 3 + a\n",
        "    lr = 1e-5\n",
        "    train_batch_size = 8\n",
        "    eval_batch_size = 4\n",
        "    train_epochs = 4\n",
        "    weight_decay = 0.01\n",
        "    warmup_ratio = 0.1\n",
        "    num_labels = 3\n",
        "    debug=True\n",
        "    model = \"microsoft/deberta-v3-xsmall\"\n",
        "    target_cols = ['winner_model_a', 'winner_model_b', 'winner_tie']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O7bEQ_MJ7VFQ"
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/code/piantic/train-lmsys-deberta-v3-starter-code/notebook\n",
        "\n",
        "def add_label(df):\n",
        "    labels = np.zeros(len(df), dtype=np.int32)\n",
        "    labels[df['winner_model_a'] == 1] = 0\n",
        "    labels[df['winner_model_b'] == 1] = 1\n",
        "    labels[df['winner_tie'] == 1] = 2\n",
        "    df['label'] = labels\n",
        "    return df\n",
        "\n",
        "\n",
        "def add_stats(df):\n",
        "    # Some stats\n",
        "    df[\"prompt_words\"] = df[\"prompt\"].apply(lambda x: x.replace('\\n', ' ').split(\" \"))\n",
        "    df[\"total_prompt_words\"] = df[\"prompt\"].apply(lambda x: len(x.split(\" \")))\n",
        "    df[\"prompt_length\"] = df[\"prompt\"].apply(lambda x: len(x))\n",
        "\n",
        "    df[\"response_a_words\"] = df[\"response_a\"].apply(lambda x: x.replace('\\n', ' ').split(\" \"))\n",
        "    df[\"total_response_a_words\"] = df[\"response_a\"].apply(lambda x: len(x.split(\" \")))\n",
        "    df[\"response_a_length\"] = df[\"response_a\"].apply(lambda x: len(x))\n",
        "\n",
        "    df[\"response_b_words\"] = df[\"response_b\"].apply(lambda x: x.replace('\\n', ' ').split(\" \"))\n",
        "    df[\"total_response_b_words\"] = df[\"response_b\"].apply(lambda x: len(x.split(\" \")))\n",
        "    df[\"response_b_length\"] = df[\"response_b\"].apply(lambda x: len(x))\n",
        "\n",
        "    return df\n",
        "\n",
        "def truncate_text(df, column_name, max_length=512):\n",
        "    df[f\"{column_name}\"] = df[column_name].str[:max_length]\n",
        "    return df\n",
        "\n",
        "train = add_label(train)\n",
        "train = add_stats(train)\n",
        "train = truncate_text(train, 'prompt')\n",
        "train = truncate_text(train, 'response_a')\n",
        "train = truncate_text(train, 'response_b')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rszIb5DR7Wrn"
      },
      "outputs": [],
      "source": [
        "class Tokenize(object):\n",
        "    def __init__(self, train, valid):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
        "        self.train = train\n",
        "        self.valid = valid\n",
        "\n",
        "    def get_dataset(self, df):\n",
        "        ds = Dataset.from_dict({\n",
        "                'id': [e for e in df['id']],\n",
        "                'prompt': [ft for ft in df['prompt']],\n",
        "                'response_a': [ft for ft in df['response_a']],\n",
        "                'response_b': [ft for ft in df['response_b']],\n",
        "                'label': [s for s in df['label']],\n",
        "            })\n",
        "        return ds\n",
        "\n",
        "    def tokenize_function(self, df):\n",
        "        tokenized_inputs = self.tokenizer(\n",
        "            df['prompt'], df['response_a'], df['response_b'],\n",
        "            truncation=True, padding=True, max_length=CFG.max_length\n",
        "        )\n",
        "        return tokenized_inputs\n",
        "\n",
        "    def __call__(self):\n",
        "        train_ds = self.get_dataset(train)\n",
        "        valid_ds = self.get_dataset(valid)\n",
        "\n",
        "        tokenized_train = train_ds.map(\n",
        "            self.tokenize_function, batched=True\n",
        "        )\n",
        "        tokenized_valid = valid_ds.map(\n",
        "            self.tokenize_function, batched=True\n",
        "        )\n",
        "\n",
        "        return tokenized_train, tokenized_valid, self.tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JOGmT_PO7YEC"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    score = log_loss(labels, predictions)\n",
        "    results = {\n",
        "        'score': score\n",
        "    }\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Op0qs1TQ7ZOG"
      },
      "outputs": [],
      "source": [
        "data = train.copy()\n",
        "data[\"label\"] = data[\"label\"].astype('int32')\n",
        "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
        "for i, (_, val_index) in enumerate(skf.split(data, data[\"label\"])):\n",
        "    data.loc[val_index, \"fold\"] = i\n",
        "data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bOn0NM1n7aTz"
      },
      "outputs": [],
      "source": [
        "if CFG.debug:\n",
        "    display(data.groupby('fold').size())\n",
        "    data = data.sample(n=1000, random_state=0).reset_index(drop=True)\n",
        "    display(data.groupby('fold').size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zuKEsSNl7bgn"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    fp16=True,\n",
        "    learning_rate=CFG.lr,\n",
        "    per_device_train_batch_size=CFG.train_batch_size,\n",
        "    per_device_eval_batch_size=CFG.eval_batch_size,\n",
        "    num_train_epochs=CFG.train_epochs,\n",
        "    weight_decay=CFG.weight_decay,\n",
        "    evaluation_strategy='epoch',\n",
        "    metric_for_best_model='score',\n",
        "    save_strategy='epoch',\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to='none',\n",
        "    warmup_ratio=CFG.warmup_ratio,\n",
        "    optim='adamw_torch'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRE2KCor7c4m"
      },
      "outputs": [],
      "source": [
        "for fold in range(len(data['fold'].unique())):\n",
        "    train = data[data['fold'] != fold]\n",
        "    valid = data[data['fold'] == fold]\n",
        "\n",
        "    # ADD NEW TOKENS for (\"\\n\") new paragraph and (\" \"*2) double space\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
        "    tokenizer.add_tokens([AddedToken(\"\\n\", normalized=False)])\n",
        "    tokenizer.add_tokens([AddedToken(\" \"*2, normalized=False)])\n",
        "    tokenize = Tokenize(train, valid)\n",
        "    tokenized_train, tokenized_valid, tokenizer = tokenize()\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(CFG.model, num_labels=CFG.num_labels)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train,\n",
        "        eval_dataset=tokenized_valid,\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    y_true = valid['label'].values\n",
        "    predictions = trainer.predict(tokenized_valid).predictions\n",
        "    predictions = predictions.argmax(axis=1)# + 1\n",
        "    cm = confusion_matrix(y_true, predictions, labels=[x for x in range(0,3)])\n",
        "    draw_cm = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                                  display_labels=[x for x in range(0,3)])\n",
        "    draw_cm.plot()\n",
        "    plt.show()\n",
        "\n",
        "    trainer.save_model(f'deberta-v3-xsmall_fold_{fold}')\n",
        "    tokenizer.save_pretrained(f'deberta-v3-xsmall_fold_{fold}')\n",
        "\n",
        "    valid.to_csv(f'valid_df_fold_{fold}.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rRBuBgD8D3C"
      },
      "outputs": [],
      "source": [
        "# 加载训练好的模型和分词器\n",
        "model = AutoModelForSequenceClassification.from_pretrained(f'deberta-v3-xsmall_fold_{fold}', num_labels=CFG.num_labels)\n",
        "tokenizer = AutoTokenizer.from_pretrained(f'deberta-v3-xsmall_fold_{fold}')\n",
        "\n",
        "# 对测试集进行分词\n",
        "test_tokenize = Tokenize(test, None)\n",
        "test_ds = test_tokenize.get_dataset(test)\n",
        "tokenized_test = test_ds.map(\n",
        "    test_tokenize.tokenize_function, batched=True\n",
        ")\n",
        "\n",
        "# 使用模型进行预测\n",
        "predictions = trainer.predict(tokenized_test).predictions\n",
        "\n",
        "# 将预测结果转换为概率\n",
        "softmax = torch.nn.Softmax(dim=-1)\n",
        "probabilities = softmax(torch.tensor(predictions)).tolist()\n",
        "\n",
        "# 创建提交文件\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': test['id'],\n",
        "    'winner_model_a': [p[0] for p in probabilities],\n",
        "    'winner_model_b': [p[1] for p in probabilities],\n",
        "    'winner_tie': [p[2] for p in probabilities]\n",
        "})\n",
        "\n",
        "# 保存提交文件\n",
        "submission_df.to_csv('submission.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}